import numpy as np
import pandas as pd
import argparse
import math
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def matrix_multiplication(a,b):
    return np.matmul(a,b)

def predictions(output_array):
    label = []
    shape = output_array.shape
    for column in range(shape[1]):
        label.append(np.argmax(output_array[:,column]))
    return label

def gradient_preactivation(grad_act,pre_act):
    shape = pre_act.shape
    second_term = activation(pre_act) * (1- activation(pre_act))
    return np.multiply(grad_act,pre_act)

def grad_activation(weights,grad_pre_act):
    weights = np.transpose(weights)
    return matrix_multiplication(weights,grad_pre_act)

def grad_biases(grad_pre_act):
    shape_grad = grad_pre_act.shape
    sum = np.zeros((shape_grad[0],1))
    for row in range(0,shape_grad[0]):
        sum[row] = np.sum(grad_pre_act[row,:])
    return (sum/shape_grad[1])
    
def grad_weights(grad_pre_act,act):
    shape_grad = grad_pre_act.shape
    act = np.transpose(act)
    shape_act = act.shape
    sum = np.zeros((shape_grad[0],shape_act[1]))
    for column in range(0,shape_grad[1]):
        reshaped_grad = grad_pre_act[:,column].reshape(shape_grad[0],1)
        reshaped_array = act[column,:].reshape(1,shape_act[1])
        mltiply = (matrix_multiplication(reshaped_grad,reshaped_array))
        sum = sum + mltiply
    return (sum/shape_grad[1])

def gradient_outermost_layer(e_l,y_hat):
    return -(e_l - y_hat)
    
def cross_entropy(y_hat,y_true):
    loss =0
    shape = y_true.shape
    for columns in range(0, shape[1]):
        for rows in range(0,shape[0]):
            if y_true[rows,columns] == 1:
                loss = loss + (-np.log(y_hat[rows,columns]))
                break
    return loss

def softmax(x):
    shape = x.shape
    counter = 0
    for column in range(0,shape[1]):
        sum = 0
        output_vector = []
        sum = np.sum(np.exp(x[:,column] - np.max(x[:,column])))
        output_vector.append(np.exp(x[:,column] - np.max(x[:,column]))/sum)

        if counter == 0:
            output_array_buffer = np.array(output_vector)
            output_array_buffer = output_array_buffer.reshape((10,1))
        else:
            output_array_counter = np.array(output_vector)
            output_array_counter = output_array_counter.reshape((10,1))
            final_predictions = np.concatenate((output_array_buffer,output_array_counter),axis = 1)
            output_array_buffer= final_predictions
        counter = counter + 1
    return final_predictions

def pre_activation(weights,biases,x,count):
    pre_act_layer_wise = dict()
    pre_act_layer_wise[count] = matrix_multiplication(weights,x)
    shape_x = x.shape
    shape_bias = biases.shape
    biases = biases.reshape(shape_bias[0])
    for i in range(0,shape_x[1]):
        pre_act_layer_wise[count][:,i] = pre_act_layer_wise[count][:,i] + biases
    return pre_act_layer_wise[count]

def activation(x):
    shape = x.shape
    for columns in range(0,shape[1]):
        try:
            x[:,columns] = 1.0 / (1 + np.exp(-x[:,columns]))
        except:
            continue
    return x
    
# Initialisation of Weights
def initialise_weights_biases(n_hidden_layers,size_hidden_layer):
    weights = dict()
    biases = dict()

    size_hidden_list = []
    size_hidden_int = []
    size_hidden_list = size_hidden_layer.split(',')

    for i in range(0,n_hidden_layers):
        size_hidden_int.append(int(size_hidden_list[i]))
    size_hidden_int.append(10)
    size_hidden_int.insert(0,100)

    for j in range(1,n_hidden_layers+2):
        np.random.seed(1234)
        weights[j] = np.random.randn(size_hidden_int[j],size_hidden_int[j-1])* np.sqrt(2/(size_hidden_int[i] + size_hidden_int[i-1]))
        biases[j] = np.random.randn(size_hidden_int[j],1)* np.sqrt(2/(size_hidden_int[i] + size_hidden_int[i-1]))

    return weights,biases

ap = argparse.ArgumentParser(description = 'Inputing Hyperparameters')
ap.add_argument('--lr','--learning_rate',required = True,type = float)
ap.add_argument('--num_hidden','--n_hidden_layers',required = True,type = int)
ap.add_argument('--sizes','--size_hidden_layer',required = True,type = str)
ap.add_argument('--epochs','--epochs',required = True,type = int)
args = ap.parse_args()

data = pd.read_csv('train.csv')
x = data.iloc[0:100,1:101]
y = data.iloc[0:100,-1]
x = x.to_numpy()
shape_x = x.shape
x = np.transpose(x)
y = y.to_numpy()
true_probablities = np.zeros((10,shape_x[0]))
for a in range(0,shape_x[0]):
    true_probablities[y[a],a] = 1

scaler = StandardScaler()
scaled = scaler.fit_transform(x)
pca = PCA(n_components = 100)
pca.fit(scaled)
scaled = pca.transform(scaled)
new_x = scaled

weights = dict()
biases = dict()

weights,biases = initialise_weights_biases(args.num_hidden,args.sizes)

for t in range(0,args.epochs):
    pre_act = dict()
    act = dict()
    grad_pre_act = dict()
    grad_wght = dict()
    grad_bs = dict()
    grad_act = dict()
    act[0] = new_x
    scaled = new_x
    # Forward Propagation
    for i in range(1,args.num_hidden+1):
        pre_act[i] = pre_activation(weights[i],biases[i],scaled,i)
        act[i] = activation(pre_act[i])
        scaled = act[i]
    pre_act[i+1] = pre_activation(weights[i+1],biases[i+1],scaled,i+1)
    output_array = softmax(pre_act[i+1])
    labels = predictions(output_array)
    loss = cross_entropy(output_array,true_probablities)

#     #Gradient Calculations
    grad_pre_act[args.num_hidden+1] = gradient_outermost_layer(true_probablities,output_array)
    for layers in range(args.num_hidden+1,0,-1):
        grad_wght[layers] = grad_weights(grad_pre_act[layers],act[layers-1])
        grad_bs[layers] = grad_biases(grad_pre_act[layers])
        if layers > 1:
            grad_act[layers-1] = grad_activation(weights[layers],grad_pre_act[layers])
            grad_pre_act[layers-1] = gradient_preactivation(grad_act[layers-1],pre_act[layers-1])

#     #Update Parameters
    for layer in range(args.num_hidden+1,0,-1):
        weights[layer] = weights[layer] - args.lr * grad_wght[layer]
        biases[layer] = biases[layer] - args.lr * grad_bs[layer]

print(labels)
